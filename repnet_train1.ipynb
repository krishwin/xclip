{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('./xclip'))\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xclip\n",
    "import xclip.models.xclip as xclip\n",
    "from xclip.utils.logger import create_logger\n",
    "logger = create_logger('./')\n",
    "import numpy as np\n",
    "import torch\n",
    "from xclip.utils.config import  get_config  \n",
    "from xclip.models.cct import load\n",
    "import xclip.datasets.build as build\n",
    "import cv2\n",
    "import timm\n",
    "import open_clip\n",
    "from xclip.clip.clip import load_vit\n",
    "import xclip.utils.tools as tools\n",
    "import time\n",
    "from xclip.models.repnet import RepNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from ./xclip/configs/countix/countix.yaml\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.config = './xclip/configs/countix/countix.yaml'\n",
    "        self.opts = None\n",
    "        self.auto_resume = False\n",
    "        self.batch_size=4\n",
    "        self.pretrained=None\n",
    "        self.resume=False\n",
    "        self.accumulation_steps=None\n",
    "        self.output=None\n",
    "        self.only_test=False\n",
    "        self.local_rank=None\n",
    "args = Args()\n",
    "config=get_config(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-12-01 07:04:33 root]\u001b[0m\u001b[33m(factory.py 296)\u001b[0m: INFO Loaded ViT-B-16 model config.\n",
      "\u001b[32m[2024-12-01 07:04:35 urllib3.connectionpool]\u001b[0m\u001b[33m(connectionpool.py 1021)\u001b[0m: DEBUG Starting new HTTPS connection (1): huggingface.co:443\n",
      "\u001b[32m[2024-12-01 07:04:35 urllib3.connectionpool]\u001b[0m\u001b[33m(connectionpool.py 474)\u001b[0m: DEBUG https://huggingface.co:443 \"HEAD /timm/vit_base_patch16_clip_224.laion400m_e31/resolve/main/open_clip_model.safetensors HTTP/1.1\" 302 0\n",
      "\u001b[32m[2024-12-01 07:04:35 root]\u001b[0m\u001b[33m(factory.py 383)\u001b[0m: INFO Loading pretrained ViT-B-16 weights (laion400m_e31).\n"
     ]
    }
   ],
   "source": [
    "embed_model = load_vit(None,'ViT-B-16', device=\"cpu\", jit=False,pretrained='laion400m_e31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_loader = build.build_counitxdataloader(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIDES = [1,2,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RepNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,eps=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)  \n",
    "critertion = torch.nn.CrossEntropyLoss()\n",
    "start_epoch, max_accuracy = 0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[68046]: Class AVFFrameReceiver is implemented in both /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/av/.dylibs/libavdevice.60.3.100.dylib (0x1496900f8) and /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/decord/.dylibs/libavdevice.58.5.100.dylib (0x1613ef010). One of the two will be used. Which one is undefined.\n",
      "objc[68046]: Class AVFAudioReceiver is implemented in both /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/av/.dylibs/libavdevice.60.3.100.dylib (0x149690148) and /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/decord/.dylibs/libavdevice.58.5.100.dylib (0x1613ef060). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01 22:01:17 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "12/01 22:01:17 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for idx, batch_data in enumerate(train_loader):\n",
    "        images = batch_data[\"imgs\"]\n",
    "        labels = batch_data[\"label\"]\n",
    "        count   = batch_data[\"count\"]\n",
    "        count_out = [0] * images.shape[0]\n",
    "        period_length = [0] * images.shape[0]\n",
    "        confidence = [0] * images.shape[0]\n",
    "        period_count = [0] * images.shape[0]\n",
    "        periodicity_score = [0]  * images.shape[0]\n",
    "        best_confidence = [0] * images.shape[0]\n",
    "        inputs = images.view(-1,images.shape[2],images.shape[3],images.shape[4])\n",
    "        with torch.no_grad():\n",
    "                outputs = embed_model(inputs)\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "count   = batch_data[\"count\"]\n",
    "count_out = [0] * images.shape[0]\n",
    "period_length = [0] * images.shape[0]\n",
    "confidence = [0] * images.shape[0]\n",
    "period_count = [0] * images.shape[0]\n",
    "periodicity_score = [0]  * images.shape[0]\n",
    "best_confidence = [0] * images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs1=outputs[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs1 = outputs1.view(images.shape[0],-1,outputs.shape[1])\n",
    "#outputs1=outputs1.split(64,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0], [0.0], [0.0], [0.0]] tensor([[2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_period_length, raw_periodicity_score, embeddings = [], [], []\n",
    "for i in range(len(outputs1)):  # Process each batch separately to avoid OOM\n",
    "                    batch_period_length, batch_periodicity, batch_embeddings = model(outputs1[i])\n",
    "                    raw_period_length.append(batch_period_length.cpu())\n",
    "                    raw_periodicity_score.append(batch_periodicity.cpu())\n",
    "                    embeddings.append(batch_embeddings.cpu()) \n",
    "            # Post-process results\n",
    "raw_period_length, raw_periodicity_score, embeddings = torch.cat(raw_period_length,1), torch.cat(raw_periodicity_score,1), torch.cat(embeddings,1)\n",
    "for i in range(raw_period_length.shape[0]):\n",
    "                confidence[i], period_length[i], period_count[i], periodicity_score[i] = model.get_counts(raw_period_length[i], raw_periodicity_score[i], 1)\n",
    "                if best_confidence[i] is None or confidence[i] > best_confidence[i]:\n",
    "                    count_out[i] = [period_count[i][-1].item()]\n",
    "                    best_confidence[i] = confidence[i]\n",
    "print(count_out,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(dist: np.ndarray, log_scale: bool = False) -> np.ndarray:\n",
    "    \"\"\"Plot the temporal self-similarity matrix into an OpenCV image.\"\"\"\n",
    "    np.fill_diagonal(dist, np.nan)\n",
    "    if log_scale:\n",
    "        dist = np.log(1 + dist)\n",
    "    dist = -dist # Invert the distance\n",
    "    zmin, zmax = np.nanmin(dist), np.nanmax(dist)\n",
    "    heatmap = (dist - zmin) / (zmax - zmin) # Normalize into [0, 1]\n",
    "    heatmap = np.nan_to_num(heatmap, nan=1)\n",
    "    heatmap = np.clip(heatmap * 255, 0, 255).astype(np.uint8)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_VIRIDIS)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist  = torch.cdist(outputs1,outputs1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-12-01 20:53:55 matplotlib]\u001b[0m\u001b[33m(__init__.py 305)\u001b[0m: DEBUG matplotlib data path: /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/matplotlib/mpl-data\n",
      "\u001b[32m[2024-12-01 20:53:55 matplotlib]\u001b[0m\u001b[33m(__init__.py 305)\u001b[0m: DEBUG CONFIGDIR=/Users/kanantharaman/.matplotlib\n",
      "\u001b[32m[2024-12-01 20:53:55 matplotlib]\u001b[0m\u001b[33m(__init__.py 1479)\u001b[0m: DEBUG interactive is False\n",
      "\u001b[32m[2024-12-01 20:53:55 matplotlib]\u001b[0m\u001b[33m(__init__.py 1480)\u001b[0m: DEBUG platform is darwin\n",
      "\u001b[32m[2024-12-01 20:53:55 matplotlib]\u001b[0m\u001b[33m(__init__.py 305)\u001b[0m: DEBUG CACHEDIR=/Users/kanantharaman/.matplotlib\n",
      "\u001b[32m[2024-12-01 20:53:55 matplotlib.font_manager]\u001b[0m\u001b[33m(font_manager.py 1543)\u001b[0m: DEBUG Using fontManager instance from /Users/kanantharaman/.matplotlib/fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 200])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[0][:64,:64].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(f'heatmap_{0}_{0}.png', plot_heatmap(dist[0][:55,:55].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs1=outputs[::3]\n",
    "outputs1 = outputs1.view(images.shape[0],-1,outputs.shape[1])\n",
    "dist  = torch.cdist(outputs1,outputs1)**2\n",
    "for i in range(4):\n",
    "        cv2.imwrite(f'heatmap_{i}_{i}.png', plot_heatmap(dist[i].numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, criterion, optimizer, lr_scheduler, train_loader, text_labels, config, logger):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    num_steps = len(train_loader)\n",
    "    batch_time = tools.AverageMeter()\n",
    "    tot_loss_meter = tools.AverageMeter()\n",
    "    \n",
    "    start = time.time()\n",
    "    end = time.time()\n",
    "    \n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        images = batch_data[\"imgs\"]\n",
    "        labels = batch_data[\"label\"]\n",
    "        count   = batch_data[\"count\"]\n",
    "        count_out = [0] * images.shape[0]\n",
    "        period_length = [0] * images.shape[0]\n",
    "        confidence = [0] * images.shape[0]\n",
    "        period_count = [0] * images.shape[0]\n",
    "        periodicity_score = [0]  * images.shape[0]\n",
    "        best_confidence = [0] * images.shape[0]\n",
    "        inputs = images.view(-1,images.shape[2],images.shape[3],images.shape[4])\n",
    "        with torch.no_grad():\n",
    "                embeds = embed_model(inputs)\n",
    "        print(inputs.shape)\n",
    "        for stride in STRIDES:\n",
    "            stride_frames = embeds[::stride]\n",
    "            outputs = stride_frames.view(images.shape[0],-1,stride_frames.shape[1])\n",
    "            outputs=outputs.split(64,dim=1)\n",
    "            raw_period_length, raw_periodicity_score, embeddings = [], [], []\n",
    "            for i in range(len(outputs)):  # Process each batch separately to avoid OOM\n",
    "                    batch_period_length, batch_periodicity, batch_embeddings = model(outputs[i])\n",
    "                    raw_period_length.append(batch_period_length.cpu())\n",
    "                    raw_periodicity_score.append(batch_periodicity.cpu())\n",
    "                    embeddings.append(batch_embeddings.cpu()) \n",
    "            # Post-process results\n",
    "            raw_period_length, raw_periodicity_score, embeddings = torch.cat(raw_period_length,1), torch.cat(raw_periodicity_score,1), torch.cat(embeddings,1)\n",
    "            for i in range(raw_period_length.shape[0]):\n",
    "                confidence[i], period_length[i], period_count[i], periodicity_score[i] = model.get_counts(raw_period_length[i], raw_periodicity_score[i], stride)\n",
    "                if best_confidence[i] is None or confidence[i] > best_confidence[i]:\n",
    "                    count_out[i] = [period_count[i][-1].item()]\n",
    "                    best_confidence[i] = confidence[i]\n",
    "    print(count_out,count)\n",
    "    loss = criterion(torch.tensor(count_out,requires_grad=True), count.float())\n",
    "    print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    total_loss += loss.item()\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[74584]: Class AVFFrameReceiver is implemented in both /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/av/.dylibs/libavdevice.60.3.100.dylib (0x14b4220f8) and /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/decord/.dylibs/libavdevice.58.5.100.dylib (0x15008f010). One of the two will be used. Which one is undefined.\n",
      "objc[74584]: Class AVFAudioReceiver is implemented in both /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/av/.dylibs/libavdevice.60.3.100.dylib (0x14b422148) and /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/decord/.dylibs/libavdevice.58.5.100.dylib (0x15008f060). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01 22:24:32 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "12/01 22:24:32 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "torch.Size([1536, 3, 224, 224])\n",
      "[[0.0], [0.0], [0.0], [0.0]] tensor([[2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [3]])\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[75485]: Class AVFFrameReceiver is implemented in both /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/av/.dylibs/libavdevice.60.3.100.dylib (0x1408060f8) and /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/decord/.dylibs/libavdevice.58.5.100.dylib (0x145473010). One of the two will be used. Which one is undefined.\n",
      "objc[75485]: Class AVFAudioReceiver is implemented in both /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/av/.dylibs/libavdevice.60.3.100.dylib (0x140806148) and /Users/kanantharaman/anaconda3/lib/python3.11/site-packages/decord/.dylibs/libavdevice.58.5.100.dylib (0x145473060). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/01 22:27:47 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "12/01 22:27:47 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "torch.Size([1536, 3, 224, 224])\n",
      "[[0.0], [0.0], [0.0], [0.0]] tensor([[2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [3]])\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "start_epoch, max_accuracy = 0, 0.0\n",
    "for epoch in range(start_epoch, 2):\n",
    "    train_one_epoch(1, model, critertion, optimizer, scheduler, train_loader, None, config, logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
